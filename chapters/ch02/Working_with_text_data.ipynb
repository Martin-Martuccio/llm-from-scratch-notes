{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95a403e",
   "metadata": {},
   "source": [
    "## Working with text data - data preparation and sampling\n",
    "In this chapter **Sebastian** describes how to prepare the text and, in general datas, which are fondumental to feed the models. This is made by different process like the *splitting phase* which gives a first raw idea to the reader about the token that is the suitable's units for handling data in next stages. In additionaly there are the real explanation about *Tokenization phase*, where texts is transformed to tokens through different algorithm like *Byte pair encoding* and finally the sliding window approach and the process to obtain vectors to feed into *LLMs*.\n",
    "\n",
    "##### Embedding: process in which (raw) datas are converted into vector format. Different format requires different embedding methods. Passing from a discrete format to a continuous format.\n",
    "\n",
    "###### note: one of the most known pretrained model is Word2Vec, a model able to generate word embeddings by predicting the context. More similar = Closer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6abfc1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "\n",
      "First row: \n",
      " I HAD always thought\n"
     ]
    }
   ],
   "source": [
    "# Example text: short story by Edith Wharton\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "print(f\"Total number of characters: {len(content)}\\n\")\n",
    "print(f\"First row: \\n {content[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "05fa8b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was']\n"
     ]
    }
   ],
   "source": [
    "# Splitting text into tokens which are units to handle text data easier\n",
    "import re\n",
    "# skipping punctuation, whitespace, ect ... \n",
    "text = re.split(r'([,.:;?_!\"()\\']|--|\\s)', content)\n",
    "res = [item.strip() for item in text if item.strip()]\n",
    "print(len(res))\n",
    "print(res[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7b7bde",
   "metadata": {},
   "source": [
    "##### The vocabulary: defines how we map each unique word/character or token to a unique integer. \n",
    "(Easy to handle and memory size is fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5dd65c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: wasn\t TokenID: 0\n",
      "Token: mysterious\t TokenID: 1\n",
      "Token: meant\t TokenID: 2\n",
      "Token: portrait\t TokenID: 3\n",
      "Token: along\t TokenID: 4\n",
      "Token: saying\t TokenID: 5\n",
      "Token: hermit\t TokenID: 6\n",
      "Token: She\t TokenID: 7\n",
      "Token: while\t TokenID: 8\n",
      "Token: hung\t TokenID: 9\n",
      "Token: because\t TokenID: 10\n"
     ]
    }
   ],
   "source": [
    "# Before tokenization, we need to have a intermidiate representation which is the tokenIDs\n",
    "\n",
    "all_words = list(set(sorted(res)))\n",
    "# special tokens to handle exceptions\n",
    "all_words.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:idx for idx,token in enumerate(all_words)}\n",
    "for idx, tok in enumerate(vocab):\n",
    "    print(f\"Token: {tok}\\t TokenID: {vocab[tok]}\")\n",
    "    if idx >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fa7794",
   "metadata": {},
   "source": [
    "##### Tokenizer Class\n",
    "Now we can implement a Class able to convert new text into TokenID, using our mapping set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a2f4011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Function to convert text to tokens and consequently to TokensID\"\"\"\n",
    "        splitted = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        res = [item.strip() for item in splitted if item.strip()]\n",
    "        res = [item if item in self.str_to_int else \"<|unk|>\" for item in res]\n",
    "        ids = [self.str_to_int[s] for s in res]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"Function to convert TokensID to tokens and consequently to text \"\"\"\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5252fff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 134, 1131, 24, 1131, 220, 485, 757, 718, 26, 1131, 24, 1131, 112, 1131, 1131]\n",
      "<|unk|>, <|unk|> to <|unk|> you! This is a <|unk|> to <|unk|> the <|unk|> <|unk|>\n"
     ]
    }
   ],
   "source": [
    "Tokenizer = Tokenizer(vocab)\n",
    "sample = \"Hi, nice to meet you! This is a test to experiment the Tokenizer Class\"\n",
    "ids = Tokenizer.encode(text = sample)\n",
    "print(ids)\n",
    "\n",
    "original = Tokenizer.decode(ids = ids)\n",
    "print(original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df744e0",
   "metadata": {},
   "source": [
    "##### note: usually special tokens addded to handle particolar cases are the following one:\n",
    " - [BOS] beginning of sequences\n",
    " - [EOS] endining of sequences\n",
    " - [PAD] padding - to ensure all texts have the same lenght."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e667a7e4",
   "metadata": {},
   "source": [
    "##### Tokenizer Class BPE\n",
    "The LLMs use more sophisticated tokenizer which are able to managing unknown tokens. BPE allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd55a416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3a5e7aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = ( \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\" \"of someunknownPlace.\")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3863456f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe876bf",
   "metadata": {},
   "source": [
    "##### Data sampling with a sliding window\n",
    "The sliding windows is an important element to distinguish input and target. This pair is essential for training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f085a614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "# MAIN CONCEPT\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "enc_sample = enc_text[50:]\n",
    "context_size = 4\n",
    "# shifiting by one position to create input-target pair\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "12afb5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      " and ---->  established\n",
      "[290, 4920] ----> 2241\n",
      " and established ---->  himself\n",
      "[290, 4920, 2241] ----> 287\n",
      " and established himself ---->  in\n",
      "[290, 4920, 2241, 287] ----> 257\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "# in inference time the model will receive only the context and it will predict the desired token\n",
    "for i in range(1, context_size+1):\n",
    "    # context\n",
    "    context = enc_sample[:i]\n",
    "    # desired token \n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "af3680eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ACER\\Documents\\LLM-Scratch\\.venv\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataset and dataloader that extract samples using a sliding window\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            # shifting by one position to create target\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            # creating tensors where input_ids[X] and target_ids[X] are the input-target pair\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "224062f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    # Create dataset class instance with pair input-target\n",
    "    dataset = GPTDataset(txt, tokenizer, max_length, stride)\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f857868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "# checking if it works\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "# iterator over dataloader\n",
    "data_iter = iter(dataloader)\n",
    "# get first batch\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f59f63",
   "metadata": {},
   "source": [
    "#### Exercise 2.2 page 91\n",
    "To develop more intuition for how the data loader works, try to run it with different settings such as max_lenght=2 and stride=2, and max_lenght=8 and stride=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057ea6c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-Scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
